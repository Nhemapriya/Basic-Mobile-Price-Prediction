{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Import the datasets**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\ndf_long = pd.read_csv('../input/mri-and-alzheimers/oasis_longitudinal.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_long.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pre-processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_long.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_long.isnull().head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nans = lambda df: df[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nans(df_long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_long.isin([0]).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_long[\"SES\"].fillna(df_long[\"SES\"].mean(), inplace=True)\ndf_long[\"MMSE\"].fillna(df_long[\"MMSE\"].mean(), inplace=True)\nnans(df_long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef outliers_z_score(ys):\n    threshold = 3\n\n    mean_y = np.mean(ys)\n    stdev_y = np.std(ys)\n    z_scores = [(y - mean_y) / stdev_y for y in ys]\n    return np.where(np.abs(z_scores) > threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_chars(field):\n    plt.figure()\n    g = None\n    if field == \"MR Delay\":\n        df_query_mri = df_long[df_long[\"MR Delay\"] > 0]\n        g = sns.countplot(df_query_mri[\"MR Delay\"])\n        g.figure.set_size_inches(18.5, 10.5)\n    else:\n        g = sns.countplot(df_long[field])\n        g.figure.set_size_inches(18.5, 10.5)\n    \nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_map = {\"Demented\": 1, \"Nondemented\": 0}\ngender_map = {\"M\": 1, \"F\": 0}\nhand_map = {\"R\": 1, \"L\": 0}\n\ndf_long['Group'] = df_long['Group'].replace(['Converted'], ['Demented'])\n\ndf_long['Group'] = df_long['Group'].map(group_map)\ndf_long['M/F'] = df_long['M/F'].map(gender_map)\ndf_long['Hand'] = df_long['Hand'].map(hand_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_chars(\"M/F\")\ngenerate_chars(\"Hand\")\ngenerate_chars(\"MR Delay\")\ngenerate_chars(\"Age\")\ngenerate_chars(\"EDUC\")\ngenerate_chars(\"SES\")\ngenerate_chars(\"MMSE\")\ngenerate_chars(\"CDR\")\ngenerate_chars(\"eTIV\")\ngenerate_chars(\"nWBV\")\ngenerate_chars(\"ASF\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_atributes = [\"M/F\", \"MR Delay\", \"Age\", \"EDUC\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\nfor item in list_atributes:\n    print(outliers_z_score(df_long[item]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 8, 5\n\ncorr_matrix = df_long.corr()\nrcParams['figure.figsize'] = 15, 10\nsns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_long.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_confusion_matrix(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n    \n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.2f'%(fp / (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.2f'%(tp / (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.2f'%((tp+tn+0.)/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.2f'%(1-fn/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.2f'%(tp/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeature_col_names = [\"M/F\", \"Age\", \"EDUC\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\npredicted_class_names = ['Group']\n\nX = df_long[feature_col_names].values\ny = df_long[predicted_class_names].values\n\nsplit_test_size = 0.30\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{0:0.2f}% para Treinamento\".format((len(X_train)/len(df_long.index)) * 100))\nprint(\"{0:0.2f}% para Testes\".format((len(X_test)/len(df_long.index)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original Demented : {0} ({1:0.2f}%)\".format(len(df_long.loc[df_long['Group'] == 1]), 100 * (len(df_long.loc[df_long['Group'] == 1]) / len(df_long))))\nprint(\"Original Nondemented : {0} ({1:0.2f}%)\".format(len(df_long.loc[df_long['Group'] == 0]), 100 * (len(df_long.loc[df_long['Group'] == 0]) / len(df_long))))\nprint(\"\")\nprint(\"Training Demented : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), 100 * (len(y_train[y_train[:] == 1]) / len(y_train))))\nprint(\"Training Nondemented : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), 100 * (len(y_train[y_train[:] == 0]) / len(y_train))))\nprint(\"\")\nprint(\"Test Demented : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), 100 * (len(y_test[y_test[:] == 1]) / len(y_test))))\nprint(\"Test Nondemented : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), 100 * (len(y_test[y_test[:] == 0]) / len(y_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000, n_features=len(feature_col_names),\n                           n_informative=3, n_redundant=0, n_repeated=0,\n                           n_classes=2, random_state=0, shuffle=False)\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_*100\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\\n\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], feature_col_names[f], importances[indices[f]]))\n\ncol_name = lambda x : feature_col_names[x]\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices], color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), list(map(col_name,range(X.shape[1]))))\nplt.xlim([-1, X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(max_depth=2, random_state=0)\nrf_model.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef report_performance(model):\n\n    model_test = model.predict(X_test)\n\n    print(\"Confusion Matrix\")\n    print(\"{0}\".format(metrics.confusion_matrix(y_test, model_test)))\n    print(\"\")\n    print(\"Classification Report\")\n    print(metrics.classification_report(y_test, model_test))\n    \n    cm = metrics.confusion_matrix(y_test, model_test)\n    show_confusion_matrix(cm, [\"Nondemented\",\"Demented\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report_performance(rf_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\"max_depth\": [2, 4, 6, 8, 12, 14],\n              \"max_features\": [9],\n              \"min_samples_split\": [2, 4, 6, 8, 10],\n              \"min_samples_leaf\": [2, 4, 6, 8, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(n_estimators=20)\n\ngrid_search = GridSearchCV(clf_rf, param_grid=param_grid, cv=10)\ngrid_search.fit(X, y)\nreport(grid_search.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model_opt = RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_depth=8, \n                                  max_features=9, min_samples_leaf=2, min_samples_split=10, random_state=0)\nrf_model_opt.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report_performance(rf_model_opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_linear_model = svm.SVC(kernel='linear')\nsvm_linear_model.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report_performance(svm_linear_model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\nparam_grid_svm = [\n  {'C': [1, 10, 1000], 'kernel': ['linear'], 'shrinking': [True, False], 'decision_function_shape': ['ovo', 'ovr', None]},\n  {'C': [1, 10, 1000], 'kernel': ['rbf'], 'degree' : [2, 4], 'gamma': [0.1, 0.01, 0.001], 'shrinking': [True, False], 'decision_function_shape': ['ovo', 'ovr', None]},  \n  {'kernel': ['poly', 'sigmoid'], 'degree' : [2, 4], 'coef0': [0, 1, 2, 3], 'shrinking': [True, False], 'decision_function_shape': ['ovo', 'ovr', None]}\n ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nclf_svm = svm.SVC()\n\ngrid_search = GridSearchCV(clf_svm, param_grid=param_grid_svm, cv=10)\ngrid_search.fit(X, y)\nreport(grid_search.cv_results_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_model_opt = svm.SVC(C=1000, gamma=0.01, kernel=\"rbf\", degree=2, shrinking=True, decision_function_shape=\"ovo\")\nsvm_model_opt.fit(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report_performance(svm_model_opt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The suggested performance was no more effective than the default values. The sensitivity to the default values of 87% of dementia and 98% for non-dementia were not achieved. In the optimized model we had 35% for Dementia and 88% for Non Dementia.\n\nBoth algorithms presented similar performance in the objective parameter of this project, which is able to adequately classify if a given subject has dementia or not, so the sensitivity is an indicator of the model evaluation. The Random Forest algorithm was slightly better, obtaining 87% and 98% sensitivity for classification of dementia and not dementia. The RandomForest algorithm, in turn, presented values of 87% and 96%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}